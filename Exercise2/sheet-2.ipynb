{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2: Stochastic Gradient Descent and Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you'll find various tasks encompassing both theoretical and coding exercises. Each exercise corresponds to a specific number of points, which are explicitly indicated within the task description.\n",
    "\n",
    "Always use the Jupyter kernel associated with the dedicated environment when compiling the notebook and completing your exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benedikt Auer, Paul Ludwig, Jannik Niebling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 1 (Theory) (15/100)\n",
    "\n",
    "### Understanding SGD\n",
    "\n",
    "- Explain the concept of stochastic gradient descent. **(5 pts.)**\n",
    "- What is the key difference between stochastic gradient descent and regular gradient descent? **(5 pts.)**\n",
    "- Discuss the advantages and disadvantages of using stochastic gradient descent. **(5 pts.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your solution here\n",
    "\n",
    "> You can write your answer in Markdown in the markdown cell below or do this in a separate document.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Stochastic gradient descent is a version of gradient descent where the full sum/average which would ordinarily be taken in the lossfunction is approximated by the average over a sample of the data. Aside from that it is a \"normal\" gradient descent algorithm which moves all parameters a certain stepsize againt their gradient in the lossfuntion to find the minimum of the loss function.\n",
    "\n",
    "b) Not all of the data is evaluated for each gradient descent step but rather only a sample which typically contains only a small fraction of the data\n",
    "\n",
    "c) disadvantage: the approximation of the whole by the sample is not perfect an introduces some noise. This however can also be an advantage if one is for example stuck in a shallow local minimum.\n",
    "\n",
    "   advantage: since the sample is much smaller than the full data, SGD is much faster to compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 2 (Theory) (15/100)\n",
    "### SGD Algorithm\n",
    "\n",
    "Consider the following pseudo-code for stochastic gradient descent:\n",
    "```plaintext\n",
    "Initialize parameters θ\n",
    "Set learning rate α\n",
    "For each iteration t = 1, 2, ..., T:\n",
    "    Randomly shuffle the training data\n",
    "    For each training example (x_i, y_i):\n",
    "        Compute the gradient: ∇_θ L(θ; x_i, y_i)\n",
    "        Update parameters: θ = θ - α * ∇_θ L(θ; x_i, y_i)\n",
    "```\n",
    "\n",
    "- Explain the steps involved in the stochastic gradient descent algorithm. **(5 pts.)**\n",
    "- Discuss the importance of randomly shuffling the training data at each iteration. **(5 pts.)**\n",
    "- How does the choice of learning rate affect the convergence of stochastic gradient descent? **(5 pts.)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your solution here\n",
    "\n",
    "> You can write your answer in Markdown in the markdown cell below or do this in a separate document.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) For each iteration of SGD we pick a random sample from the data. Then we compute the gradient of this sample and update the parameters according to the gradient descent perscription with a stepsize alpha. This is repeated T times. (we also have to initialize some stuff in the biginning but that is not really a part of the actual gradient descent)\n",
    "\n",
    "b) Randomly drawing the sample each time makes sure that SGD is a constistent estimator (here the constistent is to be undestood such that the population is not the size of the samples but the number of samples drawn in total). If the samples is the same every time the estimator is still unbiased, but not constistent due to the fact that the sample could (and most likely would be) not representative is some way.\n",
    "\n",
    "c) A higher learning rate generally leads to a faster convergence initially can prohibit actually reaching the true minimum. Similarly a low leaning rate leads to a slow convergence that is more prone to fining small local minima, but is able to more acuratly dip into those minima. Additionally for SDG the variance of the gradient estimator and thus the noise is proportional to the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 3 (Theory) (25/100)\n",
    "### Backpropagation\n",
    "\n",
    "Consider a simple neural network with a single input neuron, a single hidden neuron, and a single output neuron. The network uses sigmoid activation functions in both the hidden and output layers. \n",
    "\n",
    "The following parameters are given\n",
    "\n",
    "- Input value: $x = 0.5$\n",
    "- Output value: $y = 0.7$\n",
    "- Initial weights: \n",
    "  - $w_{\\text{hidden}} = 0.8$\n",
    "  - $w_{\\text{output}} = -0.4$\n",
    "- Initial biases: \n",
    "  - $b_{\\text{hidden}} = 0.2 $\n",
    "  - $b_{\\text{output}} = -0.6 $\n",
    "\n",
    "Perform one iteration of the backpropagation algorithm to update the weights and biases using the following steps:\n",
    "\n",
    "1. Forward pass: Compute the output of the neural network. **(5 pts.)**\n",
    "2. Compute the loss: Use the **mean squared error** loss function. **(5 pts.)**\n",
    "3. Backward pass: Compute the gradients of the loss with respect to the weights and biases. **(5 pts.)**\n",
    "4. Update the weights and biases using a learning rate of **$\\eta=0.1$**. **(5 pts.)**\n",
    "5. What are the numerical values of the updated weights? **(5 pts.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your solution here\n",
    "\n",
    "> You can write your answer in Markdown in the markdown cell below or do this in a separate document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "def sig(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def d_sig(x):\n",
    "    return sig(x)*(1-sig(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2977024822774294"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig(sig(0.5*0.8+0.2)*(-0.4)-0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16184329276574197"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sig(sig(0.5*0.8+0.2)*(-0.4)-0.6)-0.7)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.019133291399754445\n",
      "0.03826658279950889\n",
      "0.13499105343149262\n",
      "0.20907571432328623\n"
     ]
    }
   ],
   "source": [
    "d_w_hidden = d_sig(sig(0.5*0.8+0.2)*(-0.4)-0.6)*0.8*d_sig(0.5*0.8+0.2)*0.5\n",
    "\n",
    "d_b_hidden = d_sig(sig(0.5*0.8+0.2)*(-0.4)-0.6)*0.8*d_sig(0.5*0.8+0.2)\n",
    "\n",
    "d_w_output = d_sig(sig(0.5*0.8+0.2)*(-0.4)-0.6)*sig(0.5*0.8+0.2)\n",
    "\n",
    "d_b_output = d_sig(sig(0.5*0.8+0.2)*(-0.4)-0.6)\n",
    "\n",
    "print(d_w_hidden)\n",
    "print(d_b_hidden)\n",
    "print(d_w_output)\n",
    "print(d_b_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. update:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_hidden = 0.8-0.1*d_w_hidden\n",
    "\n",
    "b_hidden = 0.2-0.1*d_b_hidden\n",
    "\n",
    "w_output = -0.4-0.1*d_w_output\n",
    "\n",
    "b_output = -0.6-0.1*d_b_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7980866708600246\n",
      "0.19617334172004913\n",
      "-0.41349910534314926\n",
      "-0.6209075714323286\n"
     ]
    }
   ],
   "source": [
    "print(w_hidden)\n",
    "print(b_hidden)\n",
    "print(w_output)\n",
    "print(b_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 1 (Programming) (45/100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task (1.a)** **(10 pts.)**\n",
    "Design a dataset where each input $x$ consists of a collection of tensors, each of shape (2), generated randomly. The corresponding output $y$ is the result of a scalar function $y=\\sin^2(x)+\\cos(x)$. Implement a PyTorch Dataset class that can generate samples of the designed dataset. Ensure that it properly handles data loading, shuffling, and batching\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### NB. You can decide if you want to generate X and Y upfront and pass them in the `CustomDataset` class or if instead you want to define a method of the `class` which handles the data generation inside the class (see below for an example). Note that there are in principle many way of doing this. Feel free to experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X):\n",
    "        # ----------------\n",
    "        # Your code here\n",
    "        # ----------------\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # ----------------\n",
    "        # Your code here\n",
    "        # ----------------\n",
    "        pass\n",
    "           \n",
    "    def __len__(self):\n",
    "        # ----------------\n",
    "        # Your code here\n",
    "        # ----------------\n",
    "        pass\n",
    "            \n",
    "    def true_fn(self):\n",
    "        # ----------------\n",
    "        # Your code here\n",
    "        # ----------------\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task (1.b)** **(10 pts.)** \n",
    "Design a neural network architecture using PyTorch that can effectively learn the mapping between the input data $x$ and the output data $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# TODO: specify input and output dimensions \n",
    "# input_dim, output_dim = \n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=1000, nlayers=4, bias=False, activation=nn.ReLU):\n",
    "        super().__init__()\n",
    "        # ----------------\n",
    "        # Your code here\n",
    "        # ----------------\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ----------------\n",
    "        # Your code here\n",
    "        # ----------------\n",
    "        pass\n",
    "    \n",
    "model = NeuralNetwork(input_dim, output_dim, hidden_dim=100)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task (1.c)** **(15 pts.)** \n",
    "Train the designed neural network using the dataset generated in part **1.a**. Utilize appropriate loss function and optimization algorithm. Train the network for a sufficient number of epochs to achieve convergence. Try out different activation functions, layer sizes, and network depth. Comment on the most optimal choice obtained. Which optimizer have you used and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Parameters\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "\n",
    "X = torch.rand(100000)\n",
    "\n",
    "# Initialize dataset and dataloader\n",
    "dataset = CustomDataset(X)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = NeuralNetwork(1, 1, hidden_dim=100)\n",
    "\n",
    "# TODO: choose loss and optimizer based on what you have learned in the lecture\n",
    "# criterion = \n",
    "# optimizer = \n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in dataloader:\n",
    "        # ----------------\n",
    "        # Your code here\n",
    "        # ----------------\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task (1.a)** **(10 pts.)** \n",
    "Plot the neural network training loss and show final results on a new set of test points:\n",
    "Visualize the training loss over epochs to observe the training progress. Additionally, generate a new set of test points, pass them through the trained neural network, and display the results. Evaluate the performance of the neural network on this test set to assess its generalization ability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Part 1: Plot training loss\n",
    "\n",
    "# ----------------\n",
    "# Your code here\n",
    "# ----------------\n",
    "\n",
    "# Part 2: Test the trained model on a new set of points\n",
    "\n",
    "# ----------------\n",
    "# Your code here\n",
    "# ----------------\n",
    "\n",
    "# Plot results\n",
    "plt.scatter(X_test, y_test, label='True', color='blue', alpha=0.5)\n",
    "plt.scatter(X_test, y_pred, label='Pred', color='red', alpha=0.5)\n",
    "\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.title('True vs Predicted Values')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
