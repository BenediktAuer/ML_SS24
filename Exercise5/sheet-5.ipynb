{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 5: Overfitting and Dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you'll find various tasks encompassing both theoretical and coding exercises. Each exercise corresponds to a specific number of points, which are explicitly indicated within the task description.\n",
    "\n",
    "Always use the Jupyter kernel associated with the dedicated environment when compiling the notebook and completing your exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 1 (Theory) (20/100)\n",
    "\n",
    "### Bias Variance Tradeoff\n",
    "\n",
    "Consider the squared loss function commonly used in regression problems, defined as $L(y, \\hat{y}) = [(y + \\epsilon) - \\hat{y}]^2$, where $y$ is the true target value, $\\epsilon$ is some random noise and $\\hat{y}$ is the predicted value by the model.\n",
    "\n",
    "- **Task (1.a)** **(10 pts.)** Derive the decomposition of the expected squared error $\\mathrm{Err}=E[(y + \\epsilon - \\hat{y})^2]$ into bias, variance, and irreducible error terms (refer to the lecture for more details).\n",
    "- **Task (1.b)** **(10 pts.)** Discuss this decomposition in light of the bias-variance tradeoff in machine learning models and in the context of overfitting. What can you say of each single term in the final version you obtain?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 2 (Theory) (15/100)\n",
    "\n",
    "### Bias Variance Tradeoff (part 2)\n",
    "\n",
    "Consider a regression problem where you have a dataset consisting of $n$ data points where each point consists of a pair $(\\mathbf{x}, y)$ with $y=f(\\mathbf{x})$. You decide to fit a linear regression model to this dataset. After training the model, you evaluate its performance using mean squared error (MSE). You notice that the model has a high MSE on both the training and test datasets.\n",
    "\n",
    "- **Task (2.a)** **(5 pts.)** Explain whether the model is suffering from high bias, high variance, or both.\n",
    "- **Task (2.b)** **(5 pts.)** Propose one approach to decrease the bias and one approach to decrease the variance of the model.\n",
    "- **Task (2.c)** **(5 pts.)** Provide a justification for each proposed approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 1 (Programming) (65/100)\n",
    "\n",
    "### Dropout\n",
    "\n",
    "The code below loads the CIFAR10 dataset and splits it into train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "transform = transforms.Compose([\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root=\"./data\", train=True, \n",
    "                                             download=True, transform=transform)\n",
    "                                             \n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                           shuffle=True, num_workers=2)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root=\"./data\", train=False, \n",
    "                                             download=True, transform=transform)\n",
    "                                             \n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                           shuffle=False, num_workers=2)\n",
    "                                           \n",
    "# the CIFAR10 classes\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task (1.a)** **(10 pts.)** \n",
    "From the experience you gained in the previous excercise sheets, implement a neural network choosing arbitrary (based on your intuition) the number of layers, activation functions etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # ----------------------------------\n",
    "        #  Your code here\n",
    "        # ----------------------------------\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ----------------------------------\n",
    "        #  Your code here\n",
    "        # ----------------------------------\n",
    "        pass\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task (1.b)** **(10 pts.)** \n",
    "For the next task, we want to use the cross-entropy loss as the objective function. Implement the cross-entropy loss from scratch. This should take predicted values and true values as input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_cross_entropy(y_pred, y_true):\n",
    "    # ----------------------------------\n",
    "    #  Your code here\n",
    "    # ----------------------------------\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task (1.c)** **(20 pts.)** \n",
    "The cell below gives you an implementation for computing the loss on the test data. Implement an appropriate training loop and train the neural network for a sufficient number of epochs. How many training steps you can achieve likely depends on you laptop/device.   \n",
    "\n",
    "Print the train and test loss after every $n$ steps. In the cell below $n$ is set to 1000 but you are free to change it to any value you think is informative to appreciate the change in the loss. \n",
    "\n",
    "> Note: For the more experienced students, you can leverage GPUs (if you have them) to achieve faster computation. Checkout how to achieve this by using the `.to(device)` function in Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "def get_test_loss(net, loss_criterion, data_loader):\n",
    "  testing_loss = []\n",
    "  with torch.no_grad():\n",
    "    for data in data_loader:\n",
    "      inputs, labels = data\n",
    "      outputs = net(inputs)\n",
    "      # calculate the loss for this batch\n",
    "      loss = loss_criterion(outputs, labels)\n",
    "      # add the loss of this batch to the list\n",
    "      testing_loss.append(loss.item())\n",
    "  # calculate the average loss\n",
    "  return sum(testing_loss) / len(testing_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss, testing_loss = [], []\n",
    "running_loss = []\n",
    "i = 0\n",
    "\n",
    "## To run on GPUs\n",
    "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# \n",
    "# then call any_tensor = any_tensor.to(device)\n",
    "\n",
    "for epoch in range(150): # 150 epochs\n",
    "  for data in train_loader:\n",
    "    # TODO: get the data \n",
    "\n",
    "    # TODO: forward pass\n",
    "\n",
    "    # TODO: backward pass\n",
    "\n",
    "    # TODO: update gradients\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "      # TODO: store and print avg_train_loss and avg_train_loss every 1000 steps.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task (1.d)** **(5 pts.)** \n",
    "What are the final values for train and test loss that you achieved? What can you observe regarding e.g., overfitting? Explain (in your own words, no implementation required) how one could in principle improve one loss (or the other, or both) by tuning your network structure, e.g., using regularization, splitting data differently, tuning hyperparameters etc.?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task (1.e)** **(20 pts.)**\n",
    "\n",
    "**Task (1.e.1)** **(15 pts.)**  Pytorch has a class called `torch.nn.Dropout()` which implements Dropout, an efficient regularization technique for preventing overfitting. You can refer to the [pytorch documentation](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html) for more information about it. You have to do some hyperparameters tuning, e.g., dropout probability, amount of dropout layers etc., to identify the best setup.\n",
    "Using the dropout class, modify the neural network you implemented above in order to incorporate dropout into its structure. Once you have done that, train the modified NN again, similarly to what you have done above. \n",
    "\n",
    "**Task (1.e.2)** **(5 pts.)** What are the train and test losses now? What can you observe? Try to plot the losses as a function of epochs/training steps using `matplotlib` or `seaborn` to support your argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetDropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # ----------------------------------\n",
    "        #  Your code here\n",
    "        # ----------------------------------\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ----------------------------------\n",
    "        #  Your code here\n",
    "        # ----------------------------------\n",
    "        pass\n",
    "\n",
    "net_dropout = NetDropout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the new `net_dropout` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------\n",
    "#  Your code here\n",
    "# ----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot the results of the losses as a function of training steps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ----------------------------------\n",
    "#  Your code here\n",
    "# ----------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
