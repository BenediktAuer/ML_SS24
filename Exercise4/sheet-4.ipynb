{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 4: Loss Functions and Regularization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you'll find various tasks encompassing both theoretical and coding exercises. Each exercise corresponds to a specific number of points, which are explicitly indicated within the task description.\n",
    "\n",
    "Always use the Jupyter kernel associated with the dedicated environment when compiling the notebook and completing your exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 1 (Theory) (25/100)\n",
    "\n",
    "### Regularization techniques\n",
    "\n",
    "In the lecture you heard about different types of regularization techniques which help the training of a neural network (or a machine learning model in general) to generalize better.     \n",
    "First, write down the formulae for the $L_1$ and $L_2$ type of regularization and for a given loss function $\\mathcal{L}$ of a regression model write down its regularized version.\n",
    "Then for each of those regularization techniques, elaborate on the following aspects:        \n",
    "- **Task (1.a)** **(5 pts.)** What are the effect of $L_1$($L_2$) regularization on the model's weights $W$?\n",
    "- **Task (1.b)** **(5 pts.)** How robust is $L_1$($L_2$) regularization with respect to outliers data?\n",
    "- **Task (1.c)** **(5 pts.)** Which one of the two techniques is more computationally advantageous and why?\n",
    "- **Task (1.d)** **(5 pts.)** Which one of the two techniques is more suited for feature selection?\n",
    "\n",
    "\n",
    "> Hint: by *feature selection (extraction)* we mean the ability of selecting the most important and informative features from the available set of features. In many real-world datasets, there can be a large number of features, some of which may be irrelevant, redundant, or noisy. Including all features in the model which can induce fitting the *noise* in the training data rather than capturing the underlying patterns. Feature selection thus aims to mitigate this by selecting only the **most relevant** features, which can improve model performance, reduce complexity, and enhance interpretability.\n",
    "\n",
    "\n",
    "- **Task (1.e)** **(5 pts.)** Based on what you have learned in the lecture and further readings you may have done, discuss why one type of regularization can be more performant than the other. Based on this, in which scenarios would you recommend using one over the other?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diffrent regularization are given by\n",
    "$$\n",
    "\\mathcal{L}_{\\text{reg, }L_1} = \\mathcal{L} + \\lambda \\sum_{i=1}^{n} |w_i|\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\mathcal{L}_{\\text{reg, }L_2} = \\mathcal{L} + \\lambda \\sum_{i=1}^{n} w_i^2\n",
    "$$\n",
    "\n",
    "- Task (1.a): The $L_1$ regularization pushes most weights to zero.  This is due to the modulus of the weight. On the other hand the $L2$ regularization scales quadratic with the weight, leading to less penalization for small terms. Therfore the regularization encourages smaller weights, without forcing them to 0 ( for reasonable choices of $\\lambda$).\n",
    "\n",
    "- Task(1.b): The robustness against outliers follows directly from Task(1.a). Since $L_1$ pushes most weights to zero, it is robust against outliners. $L_2$ only enforces only small weights, so in principle outliniers are still a problem.\n",
    "\n",
    "- Task(1.c)\n",
    "$L_1$ is more advantagous for multiple reasons. First $L_1$ produces sparse tensor, which reduces the computational complexity by reducing the number of operations. Second the implementation of the modules can be realized using bitwise operations which are usually faster than floating point arithmetic (although this is probably neglactible).\n",
    "\n",
    "- Task(1.d)\n",
    "The $L_1$ regularization is more suited because it removes features completly by setting the associated weights to zero.\n",
    "\n",
    "- Task(1.d)\n",
    "The $L_1$ regularization is useful when the dataset has many redundent or irrelevant features, it can efficently eliminate them. Furthermore the $L_1$ regularization is useful when teh DataSet is noisy or have features which count as outliner features. The $L_2$ regularization is useful when the dataset consists of many features, which have roughly the same contribution. A nother realavent point is that the convergence while training should be higher for $L_2$ regularization. From a pure performance standpoint the $L_1$ regularization seems as the better bet. In most cases, however, it depends on the exact task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 2 (Theory) (10/100)\n",
    "\n",
    "### Neural Network Generalization\n",
    "\n",
    "To enable a neural network to generalize effectively with limited data, it's beneficial to ensure it's robust against small local variations. Achieving this involves constraining the gradient norm $\\vert\\frac{\\partial f}{\\partial x}\\vert$ across all input values $x$ within the domain. Given the potentially high dimensionality of the input domain, directly minimizing the gradient norm is impractical. Instead, we opt to minimize an upper bound on it, which solely relies on the model parameters.\n",
    "\n",
    "\n",
    "We instantiate a neural network comprising two layers, featuring $d$ input neurons, $h$ hidden neurons, and one output neuron. Let $W$ represent the weight matrix with dimensions $d \\times h$, and $(b_j)_{j=1}^h$ denote a set of biases. We use $W_{i,:}$ to refer to the $i$-th row of the weight matrix and $W_{:,j}$ for its $j$-th column. The neural network performs the following computation:\n",
    "$$\n",
    "\\begin{align}\n",
    "a_j &= \\max(0,W_{:,j}^\\top x + b_j) & \\text{(layer 1)}\\\\\n",
    "f(x) &= \\textstyle \\sum_j a_j & \\text{(layer 2)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The first layer detects patterns of the input data, and the second layer performs a pooling (i.e., sum) operation over these detected patterns.\n",
    "\n",
    "Show that the gradient norm of the network can be upper-bounded as:\n",
    "$$\n",
    "\\Big\\|\\frac{\\partial f}{\\partial x}\\Big\\| \\leq \\sqrt{h} \\cdot \\|W\\|_F\n",
    "$$\n",
    "\n",
    "> Suggestion: You can use the Cauchy-Schwarz inequality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 1 (Programming) (20/100)\n",
    "\n",
    "### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first part contains some test-loss surfaces and a complete training loop, to give a play ground for testing out different optimizers.\n",
    "The code is based on torch and you can find the documentation of the optimizers here: [https://pytorch.org/docs/stable/optim.html](https://pytorch.org/docs/stable/optim.html).        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, three different loss functions are provided:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss1(xy):\n",
    "    return (xy**2).sum(-1)/10.8822\n",
    "\n",
    "def loss2(xy):\n",
    "    return ((5 * xy[:,0]**2 + 0.3 * xy[:, 1]**2)/28.8377)\n",
    "\n",
    "def loss3(xy):\n",
    "    w = -20, 9, -12, 4, -3, -0.7, -0.5, 0.5, 0.3\n",
    "    x, y = xy[:, 0]*2, xy[:, 1]*3\n",
    "    return (w[0] * x + w[1] * y + w[2] * x * y + w[3] * x**2 + w[4] * y **2 + \n",
    "            w[5] * x**3 + w[6] * y**3 + 0.3 * (x**4 + y**4 - 0.99 * y**2 * x**2) +\n",
    "           torch.cos(0.3 * x + y * 0.7) * 37 + torch.sin(0.5 * y) * 19)/1175.0842"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alongside with an example training function `train_example`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ML_Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.params = torch.nn.Parameter(torch.tensor(((-1.5, -2),)))\n",
    "\n",
    "\n",
    "class Batch_loss():\n",
    "    def __init__(self, loss, bs=0):\n",
    "        self.bs = bs\n",
    "        self.loss = loss\n",
    "        torch.manual_seed(0)\n",
    "    \n",
    "    def __call__(self, params):\n",
    "        loss = self.loss(params)\n",
    "        if self.bs > 0:\n",
    "            n_noise = 40\n",
    "            x = torch.normal(0, 4, (n_noise, 2))\n",
    "            l = torch.normal(0, np.sqrt(100 / self.bs), (n_noise,))\n",
    "            ls = torch.normal(0., 1, (n_noise,))\n",
    "\n",
    "            dists = ((-torch.cdist(params.unsqueeze(0), x.unsqueeze(0))/ls.abs()).exp() * l).sum(-1).squeeze(0)\n",
    "            loss +=  dists\n",
    "        return loss\n",
    "\n",
    "\n",
    "def train_example(loss_func,  optim, optim_kwargs, bs=0, nsteps = int(5e2)):  \n",
    "    model = ML_Model()\n",
    "    optim_ = optim(model.parameters(), **optim_kwargs)\n",
    "    param_traj = [list(model.params[0].detach().cpu().numpy())]\n",
    "    \n",
    "    loss = Batch_loss(loss_func, bs=bs)\n",
    "    for i in range(nsteps):\n",
    "        train_loss = loss(model.params)\n",
    "        train_loss.backward()\n",
    "        optim_.step()\n",
    "        model.zero_grad()\n",
    "        param_traj.append(list(model.params[0].detach().cpu().numpy()))\n",
    "        #if ((i + 1) % max(nsteps//10, 1)) == 0:\n",
    "        #    print(f\"Epoch {i + 1} trainloss {train_loss.item():.2e}\")\n",
    "    bs_str = f\"batch-size: {bs}\" if bs else \"full-batch\"\n",
    "    splitter = \"'\"\n",
    "    plt.plot(*np.asarray(param_traj).T,  \n",
    "                        alpha=0.9, label=f'{str(optim).split(splitter)[1]}: {optim_kwargs} {bs_str}')\n",
    "    \n",
    "    print(f'{str(optim).split(splitter)[1]}: {optim_kwargs} {bs_str}||loss = {loss_func(model.params).item():.2f}')\n",
    "    return model.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_surface_toy(loss_func):\n",
    "    l = 4\n",
    "    x = np.linspace(-l, l, 100)\n",
    "    y = np.linspace(-l, l, 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    fig = plt.figure()\n",
    "    \n",
    "    loss = Batch_loss(loss_func, bs=0)    \n",
    "    Z = loss(torch.stack([torch.tensor(x.flatten()).float() for x in [X, Y]]).T).reshape(X.shape)\n",
    "    plt.contourf(X, Y, Z, cmap='viridis', levels=25)\n",
    "    plt.ylim(-l,l)\n",
    "    plt.xlim(-l,l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This training function takes as input the following arguments\n",
    "```python\n",
    "train_example(\n",
    "    loss_func: function, # loss function\n",
    "    optim: type, # optimizer\n",
    "    optim_kwargs: dict, # dictionary with optimizer hyperparameters\n",
    "    bs=0: int # batch size\n",
    ")\n",
    "```\n",
    "while the `plot_loss_surface` function provides you with an example of how to plot the loss surface of a given loss function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can combine all these ingredients together to get a minimal working example.\n",
    "In particular, you are asked to:\n",
    "- **Task (1.a)** **(10 pts.)** play around with the different loss functions and hyperparameters. From what you have learned in the lecture, try to come up with different scenarios, e.g., choices of hyperparameters, which lead to *good* and *bad* outcomes. \n",
    "- **Task (1.b)** **(10 pts.)** plot the results and comment on them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------\n",
    "#  Your code here\n",
    "# ----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 2 (Programming) (45/100)\n",
    "\n",
    "### Regularization\n",
    "\n",
    "This excercise will let you apply some of the concepts discussed in the lecture and in the theory part above. First and foremost, you will need to install `sklearn` as a new package in your environment.\n",
    "To do this, make sure to be in the folder where your virtual environment is and do\n",
    "\n",
    "```bash\n",
    "$ source venv-name/bin/pip3 install sklearn\n",
    "```\n",
    "\n",
    "if this has worked correctly, you can reload the kernel of the jupyter notebook and you should be able to run\n",
    "\n",
    "```python\n",
    "import sklearn\n",
    "```\n",
    "\n",
    "Should this command run without any problem (try it out in a separate python cell) you can proceed with the task!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code extracts the [Breast Cancer Wisconsin Dataset](https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data/download?datasetVersionNumber=2&ref=hackernoon.com) such that is already partitioned into training and test data. The data is normalized such that each dimension has mean 0 and variance 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284 284\n"
     ]
    }
   ],
   "source": [
    "import numpy, numpy.random\n",
    "\n",
    "import sklearn,sklearn.datasets\n",
    "\n",
    "def cancer_data():\n",
    "    \n",
    "    D = sklearn.datasets.load_breast_cancer()\n",
    "    X = D['data']\n",
    "    T = D['target']\n",
    "\n",
    "    # Partition the data\n",
    "    N = len(X)\n",
    "    perm = numpy.random.mtrand.RandomState(1).permutation(N)\n",
    "    Xtrain,Xtest = X[perm[:N//2]],X[perm[N//2:]]\n",
    "    Ttrain,Ttest = T[perm[:N//2]],T[perm[N//2:]]\n",
    "    print(len(Xtrain), len(Ttrain))\n",
    "    # Normalize input data\n",
    "    m,s = Xtrain.mean(axis=0),Xtrain.std(axis=0)+1e-9\n",
    "    for x in Xtrain,Xtest: \n",
    "        x -= m; x /= s\n",
    "\n",
    "    # Normalize targets\n",
    "    m,s = Ttrain.mean(),Ttrain.std()+1e-9\n",
    "    for t in Ttrain,Ttest: \n",
    "        t=t.astype(float)\n",
    "        t -= m; t /= s\n",
    "\n",
    "    return Xtrain,Ttrain,Xtest,Ttest\n",
    "\n",
    "\n",
    "Xtrain,Ttrain,Xtest,Ttest = cancer_data()\n",
    "\n",
    "nx = Xtrain.shape[1]\n",
    "nh = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code implements a class `NeuralNetworkRegressor`. This is a neural network with a single hidden layer of width `nh` and a `ReLU` activation function. \n",
    "The function `reg` is a regularizer which we set initially to zero (i.e. no regularizer). \n",
    "Because the dataset is small, the network is optimized in batch mode, using the Adam optimizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: The `Adam` otpimizer is an efficient optimizer widely used in Machine Learning. You'll learn more about it in the next lecture and you'll be using it in the next excercise as the default optimizer of the given `fit` function. If you want to know more about Adam and momentum based optimizer you can check [this link](https://medium.com/@vinodhb95/momentum-optimizer-6023aa445e18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy,torch,sklearn,sklearn.metrics\n",
    "from torch import nn,optim\n",
    "\n",
    "class NeuralNetworkRegressor:\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        torch.manual_seed(0)\n",
    "        \n",
    "        self.model = nn.Sequential(nn.Linear(nx,nh),nn.ReLU())\n",
    "        self.pool  = lambda y: 0.1*(y[:,:nh//2].sum(dim=1)-y[:,nh//2:].sum(dim=1))\n",
    "        self.loss  = nn.MSELoss()\n",
    "\n",
    "    def reg(self): \n",
    "        return 0\n",
    "        \n",
    "    def fit(self,X,T,nbit=10000):\n",
    "        \n",
    "        X = torch.Tensor(X)\n",
    "        T = torch.Tensor(T)\n",
    "\n",
    "        optimizer = optim.Adam(self.model.parameters(),lr=0.01)\n",
    "        for _ in range(nbit):\n",
    "            optimizer.zero_grad()\n",
    "            (self.loss(self.pool(self.model(X)),T)+self.reg()).backward()\n",
    "            optimizer.step()\n",
    "                \n",
    "    def predict(self,X):\n",
    "        return self.pool(self.model(torch.Tensor(X)))\n",
    "\n",
    "    def score(self,X,T):\n",
    "        return sklearn.metrics.r2_score(T,numpy.array(self.predict(X).data)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> N.B. The `score` function above implements the [Coefficient of determination](https://en.wikipedia.org/wiki/Coefficient_of_determination) ($R^2$ score). It determines how well a model predicts an outcome and it is bounded by 1 (e.g. perfect predictor). It is unbounded from elow as a model can, in general, be arbitrarily bad. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Task (2.a)** **(5 pts.)**\n",
    " Try to instantiate a neural network from the class above and train it. Then test its performance on test data. What do you conclude?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------\n",
    "#  Your code here\n",
    "# ----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Task (2.b)** **(25 pts.)** \n",
    "\n",
    "We want to use regularization towards improving the neural network model. We consider the following two quantities:\n",
    "- $\\|W\\|_\\text{Frob} =  \\sqrt{\\sum_{i=1}^d \\sum_{j=1}^h  w_{ij}^2}$\n",
    "- $\\text{Grad} = \\textstyle \\frac1N \\sum_{n=1}^N\\|\\nabla_{\\boldsymbol{x}}f (\\boldsymbol{x_n})\\|$\n",
    "\n",
    "Here $d$ is the number of input features, $h$ is the number of neurons in the hidden layer, and $W$ is the matrix of weights in the first layer.       \n",
    "> Note: in PyTorch, the matrix of weights is given in transposed form. \n",
    "\n",
    "In order for the model to generalize well, the last quantity ($\\text{Grad}$) should be prevented from becoming too large. We rely instead on the inequality $\\text{Grad}\\leq \\|W\\|_\\text{Frob}$​, that we can prove for this model, and will try to control the weight norms instead. \n",
    "\n",
    "- **Task (2.b.1)** **(10 pts.)** Implement the function `Frob(nn)` that computes $\\|W\\|_\\text{Frob}$​, i.e., the Froebinius norm of the model weights.\n",
    "- **Task (2.b.1)** **(10 pts.)** Implement the function `Grad(nn,X)`. This takes as input the neural network and some dataset, and returns the averaged gradient norm ($\\text{Grad}$)\n",
    "- **Task (2.b.c)** **(5 pts.)** Run the cell below. Can you describe what you observe and why? Please refer to excercise 2 from the theory part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Frob(nn):\n",
    "    # ----------------------------------\n",
    "    #  Your code here\n",
    "    # ----------------------------------\n",
    "    pass\n",
    "    \n",
    "    \n",
    "def Grad(nn,X):\n",
    "    # ----------------------------------\n",
    "    #  Your code here\n",
    "    # ----------------------------------\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code measures these three quantities before and after training the model\n",
    "# N.B. No regularization enforced here. \n",
    "\n",
    "def full_logging(name,nn):\n",
    "    return logging(name,nn) + ' | WFrob: %7.3f | Grad: %7.3f'%(Frob(nn),Grad(nn,Xtest))\n",
    "\n",
    "nnr = NeuralNetworkRegressor()\n",
    "print(full_logging('Before',nnr))\n",
    "nnr.fit(Xtrain,Ttrain)\n",
    "print(full_logging('After',nnr)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Task (2.c)** **(15 pts.)** \n",
    "\n",
    "Consider a new objective $\\mathcal{L}_{Frob}(θ)=MSE(θ)+\\lambda⋅\\|W\\|_\\text{Frob}^2$​. The first term is the original mean square error objective (see above) and  the second term is the regularization term. We hardcode the penalty coeffecient to $\\lambda=0.01$. A downside of the Frobenius norm is that it is not a very tight upper bound of the gradient, that is, penalizing it is does not penalize specifically high gradient. Instead, other useful properties of the model could be negatively affected by it.\n",
    "\n",
    "\n",
    "- **Task 2.c.1** **(10 pts.)**\n",
    "\n",
    "    Create a new regressor by reimplementing the regularization function with the Frobenius norm regularizer. You may for this task call the norm functions implemented in the question above, but this time you also need to ensure that these functions can be differentiated with respect to the weight parameters (i.e., make sure that gradients can be backpropagated through the model).\n",
    "\n",
    "- **Task 2.c.2** **(5 pts.)**       \n",
    "\n",
    "    What do you observe? What if you change the value of $\\lambda$? What can you conclude in general and how the new regressor performs compared to the old one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrobRegressor(NeuralNetworkRegressor):\n",
    "    \n",
    "    def reg(self):\n",
    "        # ----------------------------------\n",
    "        #  Your code here\n",
    "        # ----------------------------------\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnfrob = FrobRegressor()\n",
    "nnfrob.fit(Xtrain,Ttrain)\n",
    "\n",
    "print(full_logging('NN',nnr))\n",
    "print(full_logging('NN+Frob',nnfrob))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "angler-fix",
   "language": "python",
   "name": "angler-fix"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
