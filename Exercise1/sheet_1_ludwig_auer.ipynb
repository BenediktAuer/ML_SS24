{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 1: Introduction and basics of Artificial Neural Networks (ANNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handed in by Paul Ludwig and Benedikt Auer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you'll find various tasks encompassing both theoretical and coding exercises. Each exercise corresponds to a specific number of points, which are explicitly indicated within the task description.\n",
    "\n",
    "For the coding section, follow the steps outlined in the `README.md` file to install Python, create your virtual environment, and install your Jupyter kernel. (Note: You only need to do this once.)     \n",
    "Always use the Jupyter kernel associated with the dedicated environment when compiling the notebook and completing your exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 0 (Installation) (5/100)\n",
    "\n",
    "Follow the guidelines from the `README.md` file and install python. Then create a virtual environment, install the necessary packages and  install a new jupyter kernel. \n",
    "Once you are done with those steps, import the corresponding jupyter kernel and run the cell below without error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congratulations on installing your virtual environment and creating your first tensor: tensor([0, 1, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "tensor=torch.arange(5)\n",
    "print(f'Congratulations on installing your virtual environment and creating your first tensor: {tensor}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 1 (Theory) (20/100)\n",
    "\n",
    "### Understanding Activation Functions\n",
    "\n",
    "Consider a neural network with a single hidden layer and two neurons. You're tasked with implementing this neural network using a sigmoid activation function for the hidden layer neurons.\n",
    "\n",
    "- (1.a) **(5 pts.)** Define the architecture of the neural network, including the number of input nodes, hidden layer neurons, and output nodes and try to sketch it either by hand or using one of the many online tools, e.g.,  [draw.io](https://app.diagrams.net).\n",
    "- (1.b) **(5 pts.)** Write down the mathematical expressions for computing the output of each neuron in the hidden layer, including the activation function, for a a neural network with one single hidden layer.\n",
    "- (1.c) **(5 pts.)** Given a set of input values, perform forward propagation, e.g., compute the output of the neural network. (Hint: leverage the answer from **1.b**)\n",
    "- (1.d) **(5 pts.)** Propose alternative activation functions that could be used for the hidden layer neurons and discuss their advantages and disadvantages compared to the sigmoid function.\n",
    "- (1.e) **(5 pts.)** Discuss the limitations of using a sigmoid activation function, particularly regarding vanishing gradients and the tendency for neurons to saturate. (Bonus question as gradients have not been discussed yet).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> You can write your answer in Markdown in the markdown cell below or do this in a separate document.\n",
    "> #### Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1.a) There is 1 input node, 1 hidden layer neuron, and 1 output layer neuron.  Sketch: (Input $x$) -($w_0$)-> (hidden layer neuron) -($w_1$)-> (output neuron) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1.b) $f(z)= \\frac{1}{1+e^{-z}}$\n",
    "\n",
    "$y_\\text{hidden} = f(x \\cdot w_0+b_0)$\n",
    "\n",
    "$y_\\text{output} = f(y_\\text{hidden} \\cdot w_1+b_1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1.c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y_\\text{output} = f(f(x \\cdot w_0+b_0) \\cdot w_1+b_1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1.d) You could also use the \"sharp\" heavyside function. This would be closest to the biological case, however has the notable disatvantage of having a derivative of $0$ at (almost) every point thus making backpropagation ineffective thus motivating the usage of the most closely related steady function, the sigmoid. One could also use the rectified linear function (ReLu) which has the advantage of being much fast to compute both in terms of the function itself and in terms of the derivative while still introducing nonlinear behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1.e) If a neuron has a very large or very small imput, the derivative at this point of the sigmoid will be very small aswell. Since NN are often trained via gradient descent, the optimization becomes very slow in the direction of all weigths inputing into this neurons, aswell as its bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 2 (Theory) (30/100)\n",
    "\n",
    "### ANNs as general function approximators\n",
    "\n",
    "In a seminal paper, [G. Cybenko](https://web.njit.edu/~usman/courses/cs675_fall18/10.1.1.441.7873.pdf) has shown that a Neural Network (NN) with a single hidden layer can approximate arbitrary well any multivariable continuous function.\n",
    "In this excercise you are asked to show how to construct such a network for the special case of a 1D function using the ReLu activation function in the hidden layer, e.g.\n",
    "$$\n",
    "f(x) = \n",
    "\\begin{cases}\n",
    "    0 & \\text{if } x \\leq 0 \\\\\n",
    "    x & \\text{if } x > 0 \n",
    "\\end{cases}\n",
    "$$\n",
    "The NN should implement linear interpolation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 2.a **(15 pts.)**\n",
    "\n",
    "As a first step construct the neural neworks $NN_j$, where $ùëó$ is an arbitrary integer, with **three** neurons in the hidden layer that produce the function \n",
    "$$\n",
    "f_j(x) = \n",
    "\\begin{cases}\n",
    "    0 & \\text{if } x < aj \\vee x > a(j+2)  \\\\\n",
    "    w_j(\\frac{x}{a}-j) & \\text{if } aj<x<a(j+1) \\\\\n",
    "    w_j(j+2-\\frac{x}{a}) & \\text{if } a(j+1)<x<a(j+2)\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The final values for the weights are not enough, a thorough explanation/ derivation is required. \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"3layer_nn.png\" />\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> You can write your answer in Markdown in the markdown cell below or do this in a separate document.\n",
    "> #### Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 2.b **(15 pts.)**\n",
    "The NN that implements linear interpolation on a grid of lattice constant $ùëé$ (see image above) stacks horizontally several networks $NN_j$ (e.g., the same type of network as in the figure above in excercise **2.a**) and has as output $NN(ùë•)=\\sum_j NN_j(ùë•)$. Sketch the NN layout providing an appropriate colorscheme to highlight the repetition of the **same** hidden neurons repeated when stacking multiple $NN_j$ together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> You can write your answer in Markdown in the markdown cell below or do this in a separate document. You are free to draw the network by hand or use one of many online tools for drawing neural networks, see, e.g., [draw.io](https://app.diagrams.net). \n",
    "> #### Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 1 (Coding) (45/100)\n",
    "\n",
    "Throughout this course, you'll use [Pytorch](https://pytorch.org) one of the many Deep Learning Libraries on the market. \n",
    "You can check out the official Pytorch website for thorough [tutorials](https://pytorch.org/tutorials/). \n",
    "\n",
    "In the remainder of this notebook you'll have to go through some excercises to get familiar with concepts in pytorch, how to design a neural network and how to deal with tensor operations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task (1.a)** **(10 pts.)** Create two tensors, `tensor1` and `tensor2`, of shape `(5, 5)` filled with random values.      \n",
    " Perform element-wise addition, subtraction, multiplication, and division between those tensors.\n",
    "Calculate the mean and standard deviation of `tensor1`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here\n",
    "\n",
    "tensor1 = torch.rand((5,5))\n",
    "tensor2 = torch.rand((5,5))\n",
    "\n",
    "add = tensor1 + tensor2\n",
    "sub = tensor1 - tensor2\n",
    "multi = tensor1 * tensor2\n",
    "div = tensor1 / tensor2\n",
    "\n",
    "mean = torch.mean(tensor1)\n",
    "\n",
    "var = torch.var(tensor1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task (1.b)** **(10 pts.)** Create a tensor `tensor3` of shape `(4, 4)` filled with sequential numbers from 1 to 16.\n",
    "Reshape `tensor3` into a tensor of shape `(2, 8)`.\n",
    "Extract the second row and the last two columns of the reshaped tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here\n",
    "tensor3 = torch.arange(1,17).reshape((4,4))\n",
    "\n",
    "tensor3 = tensor3.reshape((2,8))\n",
    "\n",
    "row = tensor3[1,:]\n",
    "\n",
    "colums = tensor3[:,(-2,-1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task (1.c)** **(10 pts.)** Create two tensors, tensor4 and tensor5, both of shape `(3, 3)`, filled with random values.\n",
    "Concatenate `tensor4` and `tensor5` along the row dimension.        \n",
    "Stack `tensor4` and `tensor5` along a new dimension (3rd dimension)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here\n",
    "\n",
    "tensor4 = torch.rand((3,3))\n",
    "tensor5 = torch.rand((3,3))\n",
    "\n",
    "conc = torch.concatenate((tensor4, tensor5),dim=1)\n",
    "\n",
    "stack = torch.stack((tensor4,tensor5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task (1.d.1)** **(10 pts.)** Using proper inheritance define a new class `NeuralNetwork` which inherits from `torch.nn.Module`. Implement the neural network class such that it can be customized using different activation functions and hidden layers. Pytorch gives you the possibility to print whether a network has bias or not. Figure out how to do this with Pytorch and print if the network has a bias and, if Yes, print how many bias parameters it has. \n",
    "\n",
    "**Task (1.d.2)** **(5 pts.)** Print the neural network structure. How many parameters does your network have?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, s_input, layer_list):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.imput_size = s_input\n",
    "\n",
    "        size_prev = s_input\n",
    "        for size, f_act in layer_list:\n",
    "            self.layers.append(nn.Linear(size_prev, size))\n",
    "            size_prev = size # set output size as input for next layer\n",
    "            self.layers.append(f_act)\n",
    "\n",
    "\n",
    "newModel = Model(1,[(3, nn.ReLU()),(1, nn.Sigmoid())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=3, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=3, out_features=1, bias=True)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(newModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Network has Bias in both of ths layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('layers.0.weight', Parameter containing:\n",
      "tensor([[-0.8519],\n",
      "        [-0.9132],\n",
      "        [ 0.5043]], requires_grad=True))\n",
      "('layers.0.bias', Parameter containing:\n",
      "tensor([0.9673, 0.2415, 0.9932], requires_grad=True))\n",
      "('layers.2.weight', Parameter containing:\n",
      "tensor([[-0.0892, -0.4657, -0.5337]], requires_grad=True))\n",
      "('layers.2.bias', Parameter containing:\n",
      "tensor([-0.0808], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "params = newModel.named_parameters()\n",
    "for p in params:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Networt has 10 parameters, 4 of which are biases"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
