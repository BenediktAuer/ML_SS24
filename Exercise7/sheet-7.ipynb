{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 7: Learning Probability Distributions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you'll find various tasks encompassing both theoretical and coding exercises. Each exercise corresponds to a specific number of points, which are explicitly indicated within the task description.\n",
    "\n",
    "Always use the Jupyter kernel associated with the dedicated environment when compiling the notebook and completing your exercises.\n",
    "\n",
    "#### **Attention** This exercise sheet contains 10 bonus points (excercise 5.b). Furthermore, if you present your own **correct** solution to either excercises 2, 3, 4, or 5 in the tutorials, you are entitled for 10 additional bonus points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 1 (Theory) (10/100)\n",
    "\n",
    "### The KL divergence is lower bounded by zero\n",
    "\n",
    "Starting from the definition of the Gibbs inequality, show that the Kullback-Leibler divergence is a positive quantity, i.e., $\\textrm{KL}(P\\vert\\vert Q)\\geq0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 2 (Theory) (20/100)\n",
    "\n",
    "### Kullback-Leibler divergence as a limit of the Rényi (alpha) divergence\n",
    "\n",
    "The [Rényi divergence](https://en.wikipedia.org/wiki/Rényi_entropy) is a generalized notion of distance between two  probability distributions which can also be used for Variational Inference as it was shown in [this paper](https://proceedings.neurips.cc/paper_files/paper/2016/file/7750ca3559e5b8e1f44210283368fc16-Paper.pdf) from 2016. \n",
    "More specifically, the Rényi divergence of order $\\alpha$ or alpha-divergence of a distribution $P$ from a distribution $Q$ is defined to be \n",
    "\n",
    "$$ D_{\\alpha}(P \\| Q) = \\frac{1}{\\alpha - 1} \\ln \\sum_{i} \\left( p_i \\right)^{\\alpha} \\left( q_i \\right)^{1 - \\alpha} $$\n",
    "\n",
    "Show that the limit for $\\alpha\\to 1$ gives you the Kullback–Leibler divergence $ D_{\\text{KL}}(P \\| Q)$, i.e., \n",
    "$$ D_{\\text{KL}}(P \\| Q) = \\sum_{i} p_i \\ln \\frac{p_i}{q_i} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 3 (Theory) (30/100)\n",
    "\n",
    "### Differential Entropy\n",
    "\n",
    "The differential entropy between two random variables can be understood as a measure of the uncertainty or unpredictability of the joint distribution of the variables with respect to their continuous values. Unlike Shannon entropy, which deals with discrete probability distributions, differential entropy is used for continuous random variables.\n",
    "\n",
    "The differential entropy $ H(X,Y) $ between two continuous random variables $ X $ and $ Y $ with joint probability density function $ p(x,y) $ is defined as:\n",
    "\n",
    "$$ H(X,Y) = -\\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} p(x,y) \\ln(p(x,y)) \\, dx \\, dy $$\n",
    "\n",
    "where $ p(x,y) $ is the joint probability density function of $ X $ and $ Y $.\n",
    "\n",
    "Consider two variables $X$ and $Y$ having joint distribution $p(x, y)$.          \n",
    "- **Task 3.a (10 pts.)** Show that the differential entropy of this pair of variables satisfies $H(X, Y) \\leq H(X) + H(Y)$.\n",
    "- **Task 3.b (20 pts.)** Starting from the results in **3.a** prove that the equality holds if, and only if, $x$ and $y$ are statistically independent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 4 (Theory) (20/100)\n",
    "\n",
    "### Training by Forward KL\n",
    "\n",
    "Imagine you are given a dataset $\\mathbf{X}=\\{\\mathbf{x}_i\\}_{i=1}^N$ of inputs (e.g., images) being 2-d arrays $\\mathbf{x}_i\\in\\mathbb{R}^{n\\times n}$. You want to infer the distribution $p$ from which these data were sampled.      \n",
    "\n",
    "In order to do so, you train a generative model, e.g., a normalizing flow, which outputs a parametrized (variational) distribution $q_\\theta$. \n",
    "\n",
    "**Task (4.a)** **(20 pts.)** Starting from the definition of $\\textrm{KL}(p||q_\\theta)$ show that the loss function can be rewritten as an expectation value with respect to the true distribution $p$ of the likelihood of the learned distribution $q_\\theta$, i.e., \n",
    "$$\n",
    "-\\mathbb{E}_{x\\sim p}{\\left[\\ln q_\\theta(x)\\right]}.\n",
    "$$  \n",
    "and derive the corresponding gradient of the loss function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 5 (Theory) (30/100)\n",
    "\n",
    "### The Triangle Inequality\n",
    "\n",
    "The [triangle inequality](https://en.wikipedia.org/wiki/Triangle_inequality) is a property that holds for distance measures (metrics). It states that for any three points $ A $, $ B $, and $ C $, the distance from $ A $ to $ C $ should be less than or equal to the sum of the distances from $ A $ to $ B $ and from $ B $ to $ C $:\n",
    "\n",
    "$$ d(A, C) \\leq d(A, B) + d(B, C) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Task 5.a (20 pts.)** Does the KL divergence $D_{KL}(P \\parallel Q)$ fulfill the triangle inequality, i.e., is the KL divergence a true metric? To find this out, consider two  examples where probability distributions $P(x), Q(x),$ and $R(x)$ are given. For these examples, compute (numerically) if the triangle inequality is violated or fulfilled.\n",
    "> Hint: For simplicity, consider each sample $x\\in \\mathcal{X}=\\{0,1\\}$. You can start by considering the 3 following probability distributions:\n",
    "$$ P(x) = \\begin{pmatrix} 1 & 0 \\end{pmatrix}, Q(x) = \\begin{pmatrix} 0.5 & 0.5 \\end{pmatrix}, R(x) = \\begin{pmatrix} 0 & 1 \\end{pmatrix} $$\n",
    "What can you say about the triangle inequality in this case? Can you provide a second example with different probabilities? Can you say something different in this second case? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Task 5.b (10 (bonus) pts.)** Using the `pytorch` library, provide a simple implementation of your numerical example above that proves the fullfillment/violation of the triangle inequality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# TODO: Define the probability distributions\n",
    "#----------------\n",
    "# P = \n",
    "# Q = \n",
    "# R = \n",
    "#----------------\n",
    "\n",
    "# Compute KL divergence using PyTorch's built-in function\n",
    "def kl_divergence(P, Q):\n",
    "#----------------\n",
    "# TODO: Your code here\n",
    "#----------------\n",
    "\n",
    "# Compute the KL divergences\n",
    "kl_pq = kl_divergence(P, Q)\n",
    "kl_qr = kl_divergence(Q, R)\n",
    "kl_pr = kl_divergence(P, R)\n",
    "\n",
    "# Print the results\n",
    "print(f\"D_KL(P || Q): {kl_pq.item()}\")\n",
    "print(f\"D_KL(Q || R): {kl_qr.item()}\")\n",
    "print(f\"D_KL(P || R): {kl_pr.item()}\")\n",
    "\n",
    "# Check the triangle inequality\n",
    "#----------------\n",
    "# TODO: Your code here\n",
    "#----------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "angler-fix",
   "language": "python",
   "name": "angler-fix"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
